{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apollo_responses = dict()\n",
    "\n",
    "for keyword in keyword_site_urls:\n",
    "    if 'apollopharmacy' in keyword_site_urls[keyword].keys():\n",
    "        url = keyword_site_urls[keyword]['apollopharmacy']\n",
    "        # Sending a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        # Checking if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            apollo_responses[keyword] = response.text\n",
    "            print(\"HTML content fetched and saved.\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apollo_data = list()\n",
    "apollo_dict = dict()\n",
    "for keyword in apollo_responses:\n",
    "    print (keyword)\n",
    "    \n",
    "    # try:\n",
    "    soup = BeautifulSoup(apollo_responses[keyword], 'html.parser')\n",
    "\n",
    "    # images_div =  soup.find('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_pdpGallery')).find_all('img')\n",
    "    #images = ''\n",
    "    # for image in images_div:\n",
    "    #     print (image)\n",
    "    #     src_value = image['src'] if image else None\n",
    "    #     images += src_value + '\\n'\n",
    "    \n",
    "    drug_title = soup.find_all('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_title'))[0].get_text()\n",
    "    prescription_drug = soup.find_all('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_pdpTagWrapper'))[0].get_text()\n",
    "    product_info_div = soup.find('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_productInfoRoot'))\n",
    "    product_info_grids = soup.find_all('div', class_=lambda value: value and value.startswith('Grid_Item'))\n",
    "    manufacturer = product_info_grids[1].get_text()\n",
    "    \n",
    "    product_data = soup.find_all('div', class_='yh')\n",
    "    product_info_blocks = product_data[0].find_all('div', class_='RP')\n",
    "    about = ''\n",
    "    uses = ''\n",
    "    benefits = ''\n",
    "    directions = ''\n",
    "    storage = ''\n",
    "    side_effects = ''\n",
    "    for pi_block in product_info_blocks:\n",
    "        title = pi_block.find('h2').get_text()\n",
    "        desc = '\\n'.join(para.get_text() for para in pi_block.find_all('p'))\n",
    "        if not desc:\n",
    "            desc = '\\n'.join(para.get_text() for para in pi_block.find_all('li'))\n",
    "        if not desc:\n",
    "            desc = pi_block.find('div', class_='VP').get_text()\n",
    "        if 'About' in title:\n",
    "            about = desc\n",
    "        elif 'Uses' in title:\n",
    "            uses = desc\n",
    "        elif 'Benefits' in title:\n",
    "            benefits = desc\n",
    "        elif 'Directions' in title:\n",
    "            directions = desc\n",
    "        elif 'Storage' in title:\n",
    "            storage = desc\n",
    "        elif 'Side Effects' in title:\n",
    "            side_effects = desc\n",
    "    \n",
    "    try:\n",
    "        in_depth_info_blocks = product_data[1].find_all('div', class_='RP')\n",
    "        warnings = ''\n",
    "        interactions_checkers = ''\n",
    "        interactions = ''\n",
    "        habit = ''\n",
    "        diet = ''\n",
    "        advise = ''\n",
    "        for pi_block in in_depth_info_blocks:\n",
    "            title = pi_block.find('h2').get_text()\n",
    "            desc = '\\n'.join(para.get_text() for para in pi_block.find_all('p'))\n",
    "            if not desc:\n",
    "                desc = '\\n'.join(para.get_text() for para in pi_block.find_all('li'))\n",
    "            if not desc:\n",
    "                desc = pi_block.find('div', class_='VP').get_text()\n",
    "            if 'Warnings' in title:\n",
    "                warnings = desc\n",
    "            elif 'Checker' in title:\n",
    "                interactions_checkers = desc\n",
    "            elif 'Interactions' in title:\n",
    "                interactions = desc\n",
    "            elif 'Habit' in title:\n",
    "                habit = desc\n",
    "            elif 'Diet' in title:\n",
    "                diet = desc\n",
    "            elif 'Advise' in title:\n",
    "                advise = desc\n",
    "                \n",
    "    except:\n",
    "        in_depth_info = ''\n",
    "    try:\n",
    "        patients_concern = product_data[2].get_text()\n",
    "    except:\n",
    "        patients_concern = ''\n",
    "\n",
    "    try:\n",
    "        safety_text = ''\n",
    "        safety_data = soup.find('div', class_='bb').find_all('div', class_=\"W_\")\n",
    "        for row in safety_data:\n",
    "            safety_title = row.find('p', class_='Ld').get_text()\n",
    "            safety_tag = row.find('p', class_='_b').get_text()\n",
    "            safety_detail = row.find_all('p', class_='Ld')[1].get_text()\n",
    "            safety_text += 'Title:' + safety_title + '\\n' + 'Tag:' + safety_tag + '\\n' + 'Detail:' + safety_detail + '\\n\\n'\n",
    "        \n",
    "        print (safety_text)\n",
    "    except :\n",
    "        safety_text = ''\n",
    "    \n",
    "    faqs_text = ''\n",
    "    faqs_div = soup.find_all('div', class_='a')\n",
    "    for faq_div in faqs_div:\n",
    "        ques = faq_div.find('div', class_='b').get_text()\n",
    "        ans = faq_div.find('div', class_='g').get_text()\n",
    "        faqs_text += ques + '\\n' + ans + '\\n\\n'\n",
    "    \n",
    "    manufacturer_details = soup.find_all('div', class_='pJ')\n",
    "\n",
    "    apollo_row = {\n",
    "        'images': images,\n",
    "        'drug_title': drug_title,\n",
    "        'prescription_drug': prescription_drug,\n",
    "        'product_info_div': product_info_div,\n",
    "        'product_info_grids': product_info_grids,\n",
    "        'manufacturer': manufacturer,\n",
    "        'about': about,\n",
    "        'uses': uses,\n",
    "        'benefits': benefits,\n",
    "        'directions': directions,\n",
    "        'storage': storage,\n",
    "        'side_effects': side_effects,\n",
    "        'warnings': warnings,\n",
    "        'interactions': interactions,\n",
    "        'interactions_checkers': interactions_checkers,\n",
    "        'habit': habit,\n",
    "        'diet': diet,\n",
    "        'advise': advise,\n",
    "        'patients_concern': patients_concern,\n",
    "        'safety_text': safety_text,\n",
    "        'faqs_text': faqs_text,\n",
    "        'manufacturer_details': manufacturer_details\n",
    "        \n",
    "    }\n",
    "    apollo_data.append(apollo_row)\n",
    "    apollo_dict[keyword] = apollo_row\n",
    "    # break\n",
    "    \n",
    "f = open('/tmp/apollo.csv', 'w')\n",
    "writer = DictWriter(f, fieldnames = apollo_data[0].keys())\n",
    "writer.writeheader()\n",
    "writer.writerows(apollo_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
