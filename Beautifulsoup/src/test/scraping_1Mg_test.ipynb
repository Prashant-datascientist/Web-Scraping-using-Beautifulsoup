{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 mg website response\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load URLs from CSV file\n",
    "# keyword_site_urls = pd.read_csv('your_csv_file.csv')\n",
    "\n",
    "onemg_responses = dict()\n",
    "\n",
    "headers = {\n",
    "    'authority': 'www.1mg.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'cookie': 'VISITOR-ID=c042f201-ea11-4710-c7f5-680e8b432df4_acce55_1702704046',\n",
    "    'dnt': '1',\n",
    "    'referer': 'https://www.1mg.com/categories/vitamin-supplements/vitamin-d-121',\n",
    "    'sec-ch-ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "\n",
    "for keyword in keyword_site_urls:\n",
    "    if '1mg' in keyword_site_urls[keyword].keys():\n",
    "        url = keyword_site_urls[keyword]['1mg']\n",
    "        # Sending a GET request to the URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "        # Checking if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            onemg_responses[keyword] = response.text\n",
    "            print(\"HTML content fetched and saved.\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(onemg_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omemg_data = list()\n",
    "onemg_dict = dict()\n",
    "\n",
    "for keyword in onemg_responses:\n",
    "    print (keyword)\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(onemg_responses[keyword], 'html.parser')\n",
    "        \n",
    "        drug_title = soup.find_all('h1', class_=lambda value: value and value.startswith('DrugHeader__title-content'))[0].get_text()\n",
    "        drug_marketer = soup.find_all('div', class_=lambda value: value and value.startswith('DrugHeader__meta-value'))[0].get_text()\n",
    "        salt_info = soup.find_all('div', 'saltInfo')[0].get_text()\n",
    "        salt_synonms = soup.find_all('div', 'saltInfo')[1].get_text()\n",
    "        try:\n",
    "            storage = soup.find_all('div', 'saltInfo')[2].get_text()\n",
    "        except:\n",
    "            storage = salt_synonms\n",
    "            salt_synonms = ''\n",
    "        product_info = soup.find_all('div', class_=lambda value: value and value.startswith('DrugOverview__content'))[0].get_text()\n",
    "        try:\n",
    "            drug_uses_div = soup.find_all('ul', class_=lambda value: value and value.startswith('DrugOverview__list'))[0]\n",
    "            drug_uses = '\\n'.join(para.get_text() for para in drug_uses_div.find_all('li'))\n",
    "            drug_benefits_children = soup.find_all('div', class_=lambda value: value and value.startswith('ShowMoreArray__tile'))[0]\n",
    "\n",
    "            drug_benefits_div = drug_benefits_children.children\n",
    "            drug_benefits = ''\n",
    "            for child in drug_benefits_div:\n",
    "                benefit = child.get_text()\n",
    "                if benefit:\n",
    "                    drug_benefits += benefit + '\\n\\n'\n",
    "        except:\n",
    "            drug_uses = ''\n",
    "            drug_benefits = ''\n",
    "        \n",
    "        try:\n",
    "            side_effects_div = soup.find('div', id='side_effects')\n",
    "            side_effects_text = side_effects_div.find_all('div', class_=lambda value: value and value.startswith('DrugOverview__content'))[0].get_text()\n",
    "            side_effects_list_div = side_effects_div.find_all('div', class_=lambda value: value and value.startswith('DrugOverview__content'))[1]\n",
    "            side_effects_list = '\\n'.join(para.get_text() for para in side_effects_list_div.find_all('li'))\n",
    "        except:\n",
    "            side_effects_text = ''\n",
    "            side_effects_list = ''\n",
    "            \n",
    "        how_to_use = soup.find('div', id='how_to_use').get_text()\n",
    "        how_drug_works = soup.find('div', id='how_drug_works').get_text()\n",
    "        \n",
    "        safety_advice_div = soup.find('div', id='safety_advice').find_all('div', class_=lambda value: value and value.startswith('DrugOverview__content'))[0]\n",
    "        drug_overview_warnings_div = safety_advice_div.find_all('div', class_=lambda value: value and value.startswith('DrugOverview__warning-top'))\n",
    "        drug_warnings_descs = safety_advice_div.find_all('div', class_=lambda value: value and value.startswith('DrugOverview__content'))\n",
    "        \n",
    "        safety_advice = ''\n",
    "        for i, warning_div in enumerate(drug_overview_warnings_div):\n",
    "            warning_title = warning_div.find('span').get_text()\n",
    "            warning_tag = warning_div.find('div', class_=lambda value: value and value.startswith('DrugOverview__warning-tag')).get_text()\n",
    "            warning_desc = drug_warnings_descs[i].get_text()\n",
    "            safety_advice += 'Title:'+ warning_title + '\\n' + 'Tag:' + warning_tag + 'Description:' + warning_desc\n",
    "        try:\n",
    "            missed_dose = soup.find('div', id='missed_dose').get_text()\n",
    "        except:\n",
    "            missed_dose = ''\n",
    "        \n",
    "        try:\n",
    "            substitutes_div = soup.find('div', id='substitutes')\n",
    "            substitutes =  substitutes_div.find_all('div', class_=lambda value: value and value.startswith('SubstituteItem__item'))\n",
    "            substitutes_text = ''\n",
    "            for row in substitutes:\n",
    "                sub_name = row.find('div', class_=lambda value: value and value.startswith('SubstituteItem__name')).get_text()\n",
    "                sub_manc_name = row.find('div', class_=lambda value: value and value.startswith('SubstituteItem__manufacturer-name')).get_text()\n",
    "                substitutes_text += 'Substitute Name: '+ sub_name +'\\n Manufacturer Name: '+ sub_manc_name + '\\n\\n'\n",
    "        except:\n",
    "            substitutes_text = ''\n",
    "        \n",
    "        quick_tips_arr = soup.find('div', id='expert_advice').find('ul')\n",
    "        quick_tips = '\\n'.join(para.get_text() for para in quick_tips_arr.find_all('li'))\n",
    "        fact_box_left_arr = soup.find('div', id='fact_box').find_all('div', class_=lambda value: value and value.startswith('DrugFactBox__col-left'))\n",
    "        fact_box_right_arr = soup.find('div', id='fact_box').find_all('div', class_=lambda value: value and value.startswith('DrugFactBox__col-right'))\n",
    "\n",
    "        facts = ''\n",
    "        for i, fact in enumerate(fact_box_left_arr):\n",
    "            fact_tag = fact.get_text()\n",
    "            fact_detail = fact_box_right_arr[i].get_text()\n",
    "            facts += fact_tag + ':' + fact_detail + '\\n'\n",
    "        try:\n",
    "            \n",
    "            drug_interaction = soup.find('div', id='drug_interaction')\n",
    "            drug_interaction_desc = drug_interaction.find_all('div', class_=lambda value: value and value.startswith('DrugInteraction__desc'))\n",
    "            \n",
    "            drug_interaction_content = drug_interaction.find('div', class_=lambda value: value and value.startswith('DrugInteraction__content'))\n",
    "            drug_interaction_rows = drug_interaction_content.find_all('div', class_=lambda value: value and value.startswith('DrugInteraction__row'))\n",
    "\n",
    "            drug_interactions_text = ''\n",
    "            for di_row in drug_interaction_rows:\n",
    "                di_drug = di_row.find('div', class_=lambda value: value and value.startswith('DrugInteraction__drug')).get_text()\n",
    "                di_brands = di_row.find('div', class_=lambda value: value and value.startswith('DrugInteraction__brands')).get_text()\n",
    "                di_interaction = di_row.find('div', class_=lambda value: value and value.startswith('DrugInteraction__interaction')).get_text()\n",
    "                drug_interactions_text += 'Drug:' + di_drug + '\\n Brands:' + di_brands + '\\nInteraction:' + di_interaction + '\\n\\n'\n",
    "        except:\n",
    "            drug_interactions_text = ''\n",
    "        \n",
    "        faq_div = soup.find('div', id='faq')\n",
    "        faq_ques_div = faq_div.find_all('h3', class_=lambda value: value and value.startswith('Faqs__ques'))\n",
    "        faq_ans_div = faq_div.find_all('div', class_=lambda value: value and value.startswith('Faqs__ans'))\n",
    "        faqs = ''\n",
    "        for i, ques_block in enumerate(faq_ques_div):\n",
    "            ques = ques_block.get_text()\n",
    "            ans = faq_ans_div[i].get_text()\n",
    "            faqs += ques + '\\n' + ans + '\\n\\n'\n",
    "        \n",
    "        disclaimer = soup.find('div', class_=lambda value: value and value.startswith('DrugPage__auxiliary')).get_text()\n",
    "        references_div = soup.find('ol', class_=lambda value: value and value.startswith('DrugPage__reference'))\n",
    "        compliance_info =soup.find('div', class_=lambda value: value and value.startswith('DrugPage__compliance-info')).get_text()\n",
    "\n",
    "\n",
    "        onemg_row = {\n",
    "            'keyword': keyword,\n",
    "            'drug_title': drug_title,\n",
    "            'drug_marketer': drug_marketer,\n",
    "            'salt_info': salt_info,\n",
    "            'salt_synonms': salt_synonms,\n",
    "            'storage': storage,\n",
    "            'salt_synonms': salt_synonms,\n",
    "            'product_info': product_info,\n",
    "            'drug_uses':drug_uses,\n",
    "            'drug_benefits': drug_benefits,\n",
    "            'side_effects_text': side_effects_text,\n",
    "            'side_effects_list': side_effects_list,\n",
    "            'how_to_use': how_to_use,\n",
    "            'how_drug_works': how_drug_works,\n",
    "            'safety_advice': safety_advice,\n",
    "            'missed_dose': missed_dose,\n",
    "            'substitutes_text': substitutes_text,\n",
    "            'quick_tips_arr': quick_tips,\n",
    "            'facts': facts,\n",
    "            'drug_interaction_text': drug_interactions_text,\n",
    "            'faqs': faqs,\n",
    "            'disclaimer': disclaimer,\n",
    "            'references_div': references_div,\n",
    "            'compliance_info':compliance_info,\n",
    "            \n",
    "        }\n",
    "        omemg_data.append(onemg_row)\n",
    "        onemg_dict[keyword] = onemg_row\n",
    "        # print (disclaimer)\n",
    "    except Exception as e: \n",
    "        print (e)\n",
    "\n",
    "\n",
    "\n",
    "f = open('/tmp/omemg.csv', 'w')\n",
    "writer = DictWriter(f, fieldnames = omemg_data[0].keys())\n",
    "writer.writeheader()\n",
    "writer.writerows(omemg_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omemg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content fetched and saved.\n",
      "HTML content fetched and saved.\n",
      "HTML content fetched and saved.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/omemg.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 191\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# # Convert omemg_data to DataFrame\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# onemg_df = pd.DataFrame(omemg_data)\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# # Save DataFrame to CSV\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# onemg_df.to_csv('onemg_data.csv', index=False)\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/omemg.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m writer \u001b[38;5;241m=\u001b[39m DictWriter(f, fieldnames \u001b[38;5;241m=\u001b[39m omemg_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    193\u001b[0m writer\u001b[38;5;241m.\u001b[39mwriteheader()\n",
      "File \u001b[1;32md:\\ThinkByte_project\\Medibuddy_project\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/omemg.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load URLs from CSV file\n",
    "keyword_site_urls = pd.read_csv('D:\\\\ThinkByte_project\\Medibuddy_project\\data\\med_urls.csv')\n",
    "\n",
    "onemg_responses = dict()\n",
    "\n",
    "headers = {\n",
    "    'authority': 'www.1mg.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'cookie': 'VISITOR-ID=c042f201-ea11-4710-c7f5-680e8b432df4_acce55_1702704046',\n",
    "    'dnt': '1',\n",
    "    'referer': 'https://www.1mg.com/categories/vitamin-supplements/vitamin-d-121',\n",
    "    'sec-ch-ua': '\"Not_A Brand\";v=\"8\", \"Chromium\";v=\"120\", \"Google Chrome\";v=\"120\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "for index, row in keyword_site_urls.iterrows():\n",
    "    url = row['source1']\n",
    "    keyword = row.get('keyword', '')  # Get keyword if available, otherwise use empty string\n",
    "    # Sending a GET request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        onemg_responses[keyword] = response.text\n",
    "        print(\"HTML content fetched and saved.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n",
    "\n",
    "omemg_data = []\n",
    "onemg_dict = dict()\n",
    "\n",
    "for keyword in onemg_responses:\n",
    "    try:\n",
    "        soup = BeautifulSoup(onemg_responses[keyword], 'html.parser')\n",
    "        \n",
    "        drug_title = soup.find_all('h1', class_=lambda value: value and value.startswith('DrugHeader__title-content'))[0].get_text()\n",
    "        drug_marketer = soup.find_all('div', class_=lambda value: value and value.startswith('DrugHeader__meta-value'))[0].get_text()\n",
    "        salt_info = soup.find_all('div', 'saltInfo')[0].get_text()\n",
    "        salt_synonms = soup.find_all('div', 'saltInfo')[1].get_text()\n",
    "        try:\n",
    "            storage = soup.find_all('div', 'saltInfo')[2].get_text()\n",
    "        except:\n",
    "            storage = salt_synonms\n",
    "            salt_synonms = ''\n",
    "        product_info = soup.find_all('div', class_=lambda value: value and value.startswith('DrugOverview__content'))[0].get_text()\n",
    "        try:\n",
    "            drug_uses_div = soup.find_all('ul', class_=lambda value: value and value.startswith('DrugOverview__list'))[0]\n",
    "            drug_uses = '\\n'.join(para.get_text() for para in drug_uses_div.find_all('li'))\n",
    "            drug_benefits_children = soup.find_all('div', class_=lambda value: value and value.startswith('ShowMoreArray__tile'))[0]\n",
    "\n",
    "            drug_benefits_div = drug_benefits_children.children\n",
    "            drug_benefits = ''\n",
    "            for child in drug_benefits_div:\n",
    "                benefit = child.get_text()\n",
    "                if benefit:\n",
    "                    drug_benefits += benefit + '\\n\\n'\n",
    "        except:\n",
    "            drug_uses = ''\n",
    "            drug_benefits = ''\n",
    "        \n",
    "        try:\n",
    "            side_effects_div = soup.find('div', id='side_effects')\n",
    "            side_effects_text = side_effects_div.find_all('div', class_=lambda value: value and value.startswith('DrugOverview__content'))[0].get_text()\n",
    "            side_effects_list_div = side_effects_div.find_all('div', class_=lambda value: value and value.startswith('DrugOverview__content'))[1]\n",
    "            side_effects_list = '\\n'.join(para.get_text() for para in side_effects_list_div.find_all('li'))\n",
    "        except:\n",
    "            side_effects_text = ''\n",
    "            side_effects_list = ''\n",
    "            \n",
    "        how_to_use = soup.find('div', id='how_to_use').get_text()\n",
    "        how_drug_works = soup.find('div', id='how_drug_works').get_text()\n",
    "        \n",
    "        safety_advice_div = soup.find('div', id='safety_advice').find_all('div', class_=lambda value: value and value.startswith('DrugOverview__content'))[0]\n",
    "        drug_overview_warnings_div = safety_advice_div.find_all('div', class_=lambda value: value and value.startswith('DrugOverview__warning-top'))\n",
    "        drug_warnings_descs = safety_advice_div.find_all('div', class_=lambda value: value and value.startswith('DrugOverview__content'))\n",
    "        \n",
    "        safety_advice = ''\n",
    "        for i, warning_div in enumerate(drug_overview_warnings_div):\n",
    "            warning_title = warning_div.find('span').get_text()\n",
    "            warning_tag = warning_div.find('div', class_=lambda value: value and value.startswith('DrugOverview__warning-tag')).get_text()\n",
    "            warning_desc = drug_warnings_descs[i].get_text()\n",
    "            safety_advice += 'Title:'+ warning_title + '\\n' + 'Tag:' + warning_tag + 'Description:' + warning_desc\n",
    "        try:\n",
    "            missed_dose = soup.find('div', id='missed_dose').get_text()\n",
    "        except:\n",
    "            missed_dose = ''\n",
    "        \n",
    "        try:\n",
    "            substitutes_div = soup.find('div', id='substitutes')\n",
    "            substitutes =  substitutes_div.find_all('div', class_=lambda value: value and value.startswith('SubstituteItem__item'))\n",
    "            substitutes_text = ''\n",
    "            for row in substitutes:\n",
    "                sub_name = row.find('div', class_=lambda value: value and value.startswith('SubstituteItem__name')).get_text()\n",
    "                sub_manc_name = row.find('div', class_=lambda value: value and value.startswith('SubstituteItem__manufacturer-name')).get_text()\n",
    "                substitutes_text += 'Substitute Name: '+ sub_name +'\\n Manufacturer Name: '+ sub_manc_name + '\\n\\n'\n",
    "        except:\n",
    "            substitutes_text = ''\n",
    "        \n",
    "        quick_tips_arr = soup.find('div', id='expert_advice').find('ul')\n",
    "        quick_tips = '\\n'.join(para.get_text() for para in quick_tips_arr.find_all('li'))\n",
    "        fact_box_left_arr = soup.find('div', id='fact_box').find_all('div', class_=lambda value: value and value.startswith('DrugFactBox__col-left'))\n",
    "        fact_box_right_arr = soup.find('div', id='fact_box').find_all('div', class_=lambda value: value and value.startswith('DrugFactBox__col-right'))\n",
    "\n",
    "        facts = ''\n",
    "        for i, fact in enumerate(fact_box_left_arr):\n",
    "            fact_tag = fact.get_text()\n",
    "            fact_detail = fact_box_right_arr[i].get_text()\n",
    "            facts += fact_tag + ':' + fact_detail + '\\n'\n",
    "        try:\n",
    "            \n",
    "            drug_interaction = soup.find('div', id='drug_interaction')\n",
    "            drug_interaction_desc = drug_interaction.find_all('div', class_=lambda value: value and value.startswith('DrugInteraction__desc'))\n",
    "            \n",
    "            drug_interaction_content = drug_interaction.find('div', class_=lambda value: value and value.startswith('DrugInteraction__content'))\n",
    "            drug_interaction_rows = drug_interaction_content.find_all('div', class_=lambda value: value and value.startswith('DrugInteraction__row'))\n",
    "\n",
    "            drug_interactions_text = ''\n",
    "            for di_row in drug_interaction_rows:\n",
    "                di_drug = di_row.find('div', class_=lambda value: value and value.startswith('DrugInteraction__drug')).get_text()\n",
    "                di_brands = di_row.find('div', class_=lambda value: value and value.startswith('DrugInteraction__brands')).get_text()\n",
    "                di_interaction = di_row.find('div', class_=lambda value: value and value.startswith('DrugInteraction__interaction')).get_text()\n",
    "                drug_interactions_text += 'Drug:' + di_drug + '\\n Brands:' + di_brands + '\\nInteraction:' + di_interaction + '\\n\\n'\n",
    "        except:\n",
    "            drug_interactions_text = ''\n",
    "        \n",
    "        faq_div = soup.find('div', id='faq')\n",
    "        faq_ques_div = faq_div.find_all('h3', class_=lambda value: value and value.startswith('Faqs__ques'))\n",
    "        faq_ans_div = faq_div.find_all('div', class_=lambda value: value and value.startswith('Faqs__ans'))\n",
    "        faqs = ''\n",
    "        for i, ques_block in enumerate(faq_ques_div):\n",
    "            ques = ques_block.get_text()\n",
    "            ans = faq_ans_div[i].get_text()\n",
    "            faqs += ques + '\\n' + ans + '\\n\\n'\n",
    "        \n",
    "        disclaimer = soup.find('div', class_=lambda value: value and value.startswith('DrugPage__auxiliary')).get_text()\n",
    "        references_div = soup.find('ol', class_=lambda value: value and value.startswith('DrugPage__reference'))\n",
    "        compliance_info =soup.find('div', class_=lambda value: value and value.startswith('DrugPage__compliance-info')).get_text()\n",
    "\n",
    "\n",
    "        onemg_row = {\n",
    "            'keyword': keyword,\n",
    "            'drug_title': drug_title,\n",
    "            'drug_marketer': drug_marketer,\n",
    "            'salt_info': salt_info,\n",
    "            'salt_synonms': salt_synonms,\n",
    "            'storage': storage,\n",
    "            'salt_synonms': salt_synonms,\n",
    "            'product_info': product_info,\n",
    "            'drug_uses':drug_uses,\n",
    "            'drug_benefits': drug_benefits,\n",
    "            'side_effects_text': side_effects_text,\n",
    "            'side_effects_list': side_effects_list,\n",
    "            'how_to_use': how_to_use,\n",
    "            'how_drug_works': how_drug_works,\n",
    "            'safety_advice': safety_advice,\n",
    "            'missed_dose': missed_dose,\n",
    "            'substitutes_text': substitutes_text,\n",
    "            'quick_tips_arr': quick_tips,\n",
    "            'facts': facts,\n",
    "            'drug_interaction_text': drug_interactions_text,\n",
    "            'faqs': faqs,\n",
    "            'disclaimer': disclaimer,\n",
    "            'references_div': references_div,\n",
    "            'compliance_info':compliance_info,\n",
    "            \n",
    "        }\n",
    "        omemg_data.append(onemg_row)\n",
    "        onemg_dict[keyword] = onemg_row\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Convert omemg_data to DataFrame\n",
    "onemg_df = pd.DataFrame(omemg_data)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "onemg_df.to_csv('/data/onemg_data.csv', index=False)\n",
    "        \n",
    "# f = open('/tmp/omemg.csv', 'w')\n",
    "# writer = DictWriter(f, fieldnames = omemg_data[0].keys())\n",
    "# writer.writeheader()\n",
    "# writer.writerows(omemg_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
