{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from csv import DictWriter\n",
    "\n",
    "# Step 1: Read URLs from CSV file\n",
    "keyword_site_urls = dict()\n",
    "with open('D:\\\\ThinkByte_project\\Medibuddy_project\\data\\med_urls.csv', 'r') as csvfile:\n",
    "    csvreader = csv.DictReader(csvfile)\n",
    "    for row in csvreader:\n",
    "        keyword = row['source3']  # 'source3' is the column containing URLs\n",
    "        keyword_site_urls[keyword] = {'source3': row['source3']}  # Adjust as per your CSV structure\n",
    "\n",
    "print(\"CSV data\",keyword_site_urls)\n",
    "\n",
    "apollo_responses = dict()\n",
    "\n",
    "# Step 2: Fetch data from each URL\n",
    "for keyword in keyword_site_urls:\n",
    "    url = keyword_site_urls[keyword]['source3']\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        apollo_responses[keyword] = response.text\n",
    "        print(f\"HTML content fetched for {keyword}.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch webpage for {keyword}. Status code: {response.status_code}\")\n",
    "print(\"responce\",apollo_responses)\n",
    "# Step 3: Process fetched data\n",
    "apollo_data = []\n",
    "for keyword in apollo_responses:\n",
    "    soup = BeautifulSoup(apollo_responses[keyword], 'html.parser')\n",
    "    # Your data processing logic here\n",
    "\n",
    "    drug_title = soup.find_all('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_title'))[0].text.strip()\n",
    "    prescription_drug = soup.find_all('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_pdpTagWrapper'))[0].get_text()\n",
    "    product_info_div = soup.find('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_productInfoRoot'))\n",
    "    product_info_grids = soup.find_all('div', class_=lambda value: value and value.startswith('Grid_Item'))\n",
    "    manufacturer = product_info_grids[1].get_text()\n",
    "    \n",
    "    product_data = soup.find_all('div', class_='yh')\n",
    "    product_info_blocks = product_data[0].find_all('div', class_='RP')\n",
    "    about = ''\n",
    "    uses = ''\n",
    "    benefits = ''\n",
    "    directions = ''\n",
    "    storage = ''\n",
    "    side_effects = ''\n",
    "    for pi_block in product_info_blocks:\n",
    "        title = pi_block.find('h2').get_text()\n",
    "        desc = '\\n'.join(para.get_text() for para in pi_block.find_all('p'))\n",
    "        if not desc:\n",
    "            desc = '\\n'.join(para.get_text() for para in pi_block.find_all('li'))\n",
    "        if not desc:\n",
    "            desc = pi_block.find('div', class_='VP').get_text()\n",
    "        if 'About' in title:\n",
    "            about = desc\n",
    "        elif 'Uses' in title:\n",
    "            uses = desc\n",
    "        elif 'Benefits' in title:\n",
    "            benefits = desc\n",
    "        elif 'Directions' in title:\n",
    "            directions = desc\n",
    "        elif 'Storage' in title:\n",
    "            storage = desc\n",
    "        elif 'Side Effects' in title:\n",
    "            side_effects = desc\n",
    "    \n",
    "    try:\n",
    "        in_depth_info_blocks = product_data[1].find_all('div', class_='RP')\n",
    "        warnings = ''\n",
    "        interactions_checkers = ''\n",
    "        interactions = ''\n",
    "        habit = ''\n",
    "        diet = ''\n",
    "        advise = ''\n",
    "        for pi_block in in_depth_info_blocks:\n",
    "            title = pi_block.find('h2').get_text()\n",
    "            desc = '\\n'.join(para.get_text() for para in pi_block.find_all('p'))\n",
    "            if not desc:\n",
    "                desc = '\\n'.join(para.get_text() for para in pi_block.find_all('li'))\n",
    "            if not desc:\n",
    "                desc = pi_block.find('div', class_='VP').get_text()\n",
    "            if 'Warnings' in title:\n",
    "                warnings = desc\n",
    "            elif 'Checker' in title:\n",
    "                interactions_checkers = desc\n",
    "            elif 'Interactions' in title:\n",
    "                interactions = desc\n",
    "            elif 'Habit' in title:\n",
    "                habit = desc\n",
    "            elif 'Diet' in title:\n",
    "                diet = desc\n",
    "            elif 'Advise' in title:\n",
    "                advise = desc\n",
    "                \n",
    "    except:\n",
    "        in_depth_info = ''\n",
    "    try:\n",
    "        patients_concern = product_data[2].get_text()\n",
    "    except:\n",
    "        patients_concern = ''\n",
    "\n",
    "    try:\n",
    "        safety_text = ''\n",
    "        safety_data = soup.find('div', class_='bb').find_all('div', class_=\"W_\")\n",
    "        for row in safety_data:\n",
    "            safety_title = row.find('p', class_='Ld').get_text()\n",
    "            safety_tag = row.find('p', class_='_b').get_text()\n",
    "            safety_detail = row.find_all('p', class_='Ld')[1].get_text()\n",
    "            safety_text += 'Title:' + safety_title + '\\n' + 'Tag:' + safety_tag + '\\n' + 'Detail:' + safety_detail + '\\n\\n'\n",
    "        \n",
    "        print (safety_text)\n",
    "    except :\n",
    "        safety_text = ''\n",
    "    \n",
    "    faqs_text = ''\n",
    "    faqs_div = soup.find_all('div', class_='a')\n",
    "    for faq_div in faqs_div:\n",
    "        ques = faq_div.find('div', class_='b').get_text()\n",
    "        ans = faq_div.find('div', class_='g').get_text()\n",
    "        faqs_text += ques + '\\n' + ans + '\\n\\n'\n",
    "    \n",
    "    manufacturer_details = soup.find_all('div', class_='pJ')\n",
    "\n",
    "    apollo_row = {\n",
    "        # 'images': images,\n",
    "        'drug_title': drug_title,\n",
    "        'prescription_drug': prescription_drug,\n",
    "        'product_info_div': product_info_div,\n",
    "        'product_info_grids': product_info_grids,\n",
    "        'manufacturer': manufacturer,\n",
    "        'about': about,\n",
    "        'uses': uses,\n",
    "        'benefits': benefits,\n",
    "        'directions': directions,\n",
    "        'storage': storage,\n",
    "        'side_effects': side_effects,\n",
    "        'warnings': warnings,\n",
    "        'interactions': interactions,\n",
    "        'interactions_checkers': interactions_checkers,\n",
    "        'habit': habit,\n",
    "        'diet': diet,\n",
    "        'advise': advise,\n",
    "        'patients_concern': patients_concern,\n",
    "        'safety_text': safety_text,\n",
    "        'faqs_text': faqs_text,\n",
    "        'manufacturer_details': manufacturer_details\n",
    "        \n",
    "    }\n",
    "    apollo_data.append(apollo_row)\n",
    "    apollo_dict[keyword] = apollo_row\n",
    "    # break\n",
    "\n",
    "# Step 4: Save processed data into a new CSV file\n",
    "with open('apollo_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['drug_title', 'prescription_drug', 'product_info_div','product_info_grids','manufacturer','about','uses','benefits','directions','storage','side_effects','warnings','interactions','interactions_checkers','habit','diet','advise','patients_concern','safety_text','faqs_text','manufacturer_details']  # Update with your field names\n",
    "    writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(apollo_data)\n",
    "\n",
    "print(\"Data saved to apollo_data.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from csv import DictWriter\n",
    "\n",
    "# Step 1: Read URLs from CSV file\n",
    "keyword_site_urls = dict()\n",
    "with open('D:\\\\ThinkByte_project\\Medibuddy_project\\data\\med_urls.csv', 'r') as csvfile:\n",
    "    csvreader = csv.DictReader(csvfile)\n",
    "    for row in csvreader:\n",
    "        keyword = row['source3'][1]  # 'source3' is the column containing URLs\n",
    "        keyword_site_urls[keyword] = {'source3': row['source3']}  # Adjust as per your CSV structure\n",
    "\n",
    "apollo_responses = dict()\n",
    "\n",
    "# Step 2: Fetch data from each URL\n",
    "for keyword in keyword_site_urls:\n",
    "    url = keyword_site_urls[keyword]['source3']\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        apollo_responses[keyword] = response.text\n",
    "        print(f\"HTML content fetched for {keyword}.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch webpage for {keyword}. Status code: {response.status_code}\")\n",
    "\n",
    "# Step 3: Process fetched data\n",
    "apollo_data = []\n",
    "\n",
    "for keyword in apollo_responses:\n",
    "    try:\n",
    "        soup = BeautifulSoup(apollo_responses[keyword], 'html.parser')\n",
    "        drug_title = soup.find_all('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_title'))[0].get_text()\n",
    "        prescription_drug = soup.find_all('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_pdpTagWrapper'))[0].get_text()\n",
    "        product_info_div = soup.find('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_productInfoRoot'))\n",
    "        product_info_grids = soup.find_all('div', class_=lambda value: value and value.startswith('Grid_Item'))\n",
    "        manufacturer = product_info_grids[1].get_text()\n",
    "        \n",
    "        product_data = soup.find_all('div', class_='yh')\n",
    "        print(product_data)\n",
    "        ##############################\n",
    "        try:\n",
    "            product_info_blocks = product_data[0].find_all('div', class_='RP')\n",
    "            about = ''\n",
    "            uses = ''\n",
    "            benefits = ''\n",
    "            directions = ''\n",
    "            storage = ''\n",
    "            side_effects = ''\n",
    "            for pi_block in product_info_blocks:\n",
    "                title = pi_block.find('h2').get_text()\n",
    "                desc = '\\n'.join(para.get_text() for para in pi_block.find_all('p'))\n",
    "                if not desc:\n",
    "                    desc = '\\n'.join(para.get_text() for para in pi_block.find_all('li'))\n",
    "                if not desc:\n",
    "                    desc = pi_block.find('div', class_='VP').get_text()\n",
    "                if 'About' in title:\n",
    "                    about = desc\n",
    "                elif 'Uses' in title:\n",
    "                    uses = desc\n",
    "                elif 'Benefits' in title:\n",
    "                    benefits = desc\n",
    "                elif 'Directions' in title:\n",
    "                    directions = desc\n",
    "                elif 'Storage' in title:\n",
    "                    storage = desc\n",
    "                elif 'Side Effects' in title:\n",
    "                    side_effects = desc\n",
    "        except Exception as e:\n",
    "            print(e)  \n",
    "        ####################################  \n",
    "        try:\n",
    "            in_depth_info_blocks = product_data[1].find_all('div', class_='RP')\n",
    "            warnings = ''\n",
    "            interactions_checkers = ''\n",
    "            interactions = ''\n",
    "            habit = ''\n",
    "            diet = ''\n",
    "            advise = ''\n",
    "            for pi_block in in_depth_info_blocks:\n",
    "                title = pi_block.find('h2').get_text()\n",
    "                desc = '\\n'.join(para.get_text() for para in pi_block.find_all('p'))\n",
    "                if not desc:\n",
    "                    desc = '\\n'.join(para.get_text() for para in pi_block.find_all('li'))\n",
    "                if not desc:\n",
    "                    desc = pi_block.find('div', class_='VP').get_text()\n",
    "                if 'Warnings' in title:\n",
    "                    warnings = desc\n",
    "                elif 'Checker' in title:\n",
    "                    interactions_checkers = desc\n",
    "                elif 'Interactions' in title:\n",
    "                    interactions = desc\n",
    "                elif 'Habit' in title:\n",
    "                    habit = desc\n",
    "                elif 'Diet' in title:\n",
    "                    diet = desc\n",
    "                elif 'Advise' in title:\n",
    "                    advise = desc\n",
    "                    \n",
    "        except:\n",
    "            in_depth_info = ''\n",
    "        try:\n",
    "            patients_concern = product_data[2].get_text()\n",
    "        except:\n",
    "            patients_concern = ''\n",
    "\n",
    "        try:\n",
    "            safety_text = ''\n",
    "            safety_data = soup.find('div', class_='bb').find_all('div', class_=\"W_\")\n",
    "            for row in safety_data:\n",
    "                safety_title = row.find('p', class_='Ld').get_text()\n",
    "                safety_tag = row.find('p', class_='_b').get_text()\n",
    "                safety_detail = row.find_all('p', class_='Ld')[1].get_text()\n",
    "                safety_text += 'Title:' + safety_title + '\\n' + 'Tag:' + safety_tag + '\\n' + 'Detail:' + safety_detail + '\\n\\n'\n",
    "            \n",
    "            print (safety_text)\n",
    "        except :\n",
    "            safety_text = ''\n",
    "        \n",
    "        faqs_text = ''\n",
    "        faqs_div = soup.find_all('div', class_='a')\n",
    "        for faq_div in faqs_div:\n",
    "            ques = faq_div.find('div', class_='b').get_text()\n",
    "            ans = faq_div.find('div', class_='g').get_text()\n",
    "            faqs_text += ques + '\\n' + ans + '\\n\\n'\n",
    "        \n",
    "        manufacturer_details = soup.find_all('div', class_='pJ')\n",
    "\n",
    "        apollo_row = {\n",
    "         \n",
    "            'drug_title': drug_title,\n",
    "            'prescription_drug': prescription_drug,\n",
    "            'product_info_div': product_info_div,\n",
    "            'product_info_grids': product_info_grids,\n",
    "            'manufacturer': manufacturer,\n",
    "            'about': about,\n",
    "            'uses': uses,\n",
    "            'benefits': benefits,\n",
    "            'directions': directions,\n",
    "            'storage': storage,\n",
    "            'side_effects': side_effects,\n",
    "            'warnings': warnings,\n",
    "            'interactions': interactions,\n",
    "            'interactions_checkers': interactions_checkers,\n",
    "            'habit': habit,\n",
    "            'diet': diet,\n",
    "            'advise': advise,\n",
    "            'patients_concern': patients_concern,\n",
    "            'safety_text': safety_text,\n",
    "            'faqs_text': faqs_text,\n",
    "            'manufacturer_details': manufacturer_details\n",
    "            \n",
    "        }\n",
    "        apollo_data.append(apollo_row)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "\n",
    "# Step 4: Save processed data into a new CSV file\n",
    "f = open('apollo.csv', 'w')\n",
    "writer = DictWriter(f, fieldnames = apollo_data[0].keys())\n",
    "writer.writeheader()\n",
    "writer.writerows(apollo_data)\n",
    "\n",
    "print(\"Data saved to netmeds_data.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from csv import DictWriter\n",
    "\n",
    "# Step 1: Read URLs from CSV file\n",
    "keyword_site_urls = dict()\n",
    "with open('D:\\\\ThinkByte_project\\Medibuddy_project\\data\\med_urls.csv', 'r') as csvfile:\n",
    "    csvreader = csv.DictReader(csvfile)\n",
    "    for row in csvreader:\n",
    "        keyword = row['source3']  # 'source3' is the column containing URLs\n",
    "        keyword_site_urls[keyword] = {'source3': row['source3']}  # Adjust as per your CSV structure\n",
    "\n",
    "apollo_responses = dict()\n",
    "print(keyword_site_urls)\n",
    "\n",
    "# Step 2: Fetch data from each URL\n",
    "for keyword in keyword_site_urls:\n",
    "    url = keyword_site_urls[keyword]['source3']\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        apollo_responses[keyword] = response.text\n",
    "        print(f\"HTML content fetched for {keyword}.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch webpage for {keyword}. Status code: {response.status_code}\")\n",
    "\n",
    "# Step 3: Process fetched data\n",
    "apollo_data = []\n",
    "\n",
    "for keyword in apollo_responses:\n",
    "    try:\n",
    "        soup = BeautifulSoup(apollo_responses[keyword], 'html.parser')\n",
    "        drug_title = soup.find('div', class_='PdpImagePlaceholder_title__kN_jC').find('div', class_='OJ').find('h1').get_text()\n",
    "        \n",
    "        prescription_drug = soup.find_all('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_pdpTagWrapper'))[0].get_text()\n",
    "        product_info_div = soup.find('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_productInfoRoot'))\n",
    "        product_info_grids = soup.find_all('div', class_=lambda value: value and value.startswith('Grid_Item'))\n",
    "        manufacturer = product_info_grids[1].get_text()\n",
    "        \n",
    "        product_data = soup.find_all('div', class_='yh')\n",
    "        \n",
    "        print(drug_title)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://www.apollopharmacy.in/medicine/benadryl-cough-formula-syrup-450ml\"\n",
    "\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "for i in soup.find_all(\"code\"):\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m drug_title \u001b[38;5;241m=\u001b[39m drug_title \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPdpImagePlaceholder_title\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(drug_title)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "drug_title = drug_title = soup.find_all('div', class_=lambda value: value and value.startswith('PdpImagePlaceholder_title'))[0].text.strip()\n",
    "print(drug_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
