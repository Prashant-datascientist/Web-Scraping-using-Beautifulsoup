{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from csv import DictWriter\n",
    "\n",
    "# Step 1: Read URLs from CSV file\n",
    "keyword_site_urls = dict()\n",
    "with open('D:\\\\ThinkByte_project\\Medibuddy_project\\data\\med_urls.csv', 'r') as csvfile:\n",
    "    csvreader = csv.DictReader(csvfile)\n",
    "    for row in csvreader:\n",
    "        keyword = row['source2']  # 'source3' is the column containing URLs\n",
    "        keyword_site_urls[keyword] = {'source2': row['source2']}  # Adjust as per your CSV structure\n",
    "\n",
    "netmed_responses = dict()\n",
    "\n",
    "# Step 2: Fetch data from each URL\n",
    "for keyword in keyword_site_urls:\n",
    "    url = keyword_site_urls[keyword]['source2']\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        netmed_responses[keyword] = response.text\n",
    "        print(f\"HTML content fetched for {keyword}.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch webpage for {keyword}. Status code: {response.status_code}\")\n",
    "\n",
    "# Step 3: Process fetched data\n",
    "netmeds_data = []\n",
    "\n",
    "def get_li_p_text(block):\n",
    "    block_text = ''\n",
    "    for ddesc in block.children:\n",
    "        if ddesc.name == 'p':\n",
    "            block_text += '\\n' + ddesc.get_text() + '\\n'\n",
    "        elif ddesc.name == 'ul':\n",
    "            block_text += '\\n'.join(para.get_text() for para in ddesc.find_all('li'))\n",
    "        elif ddesc.name == 'table':\n",
    "            table_data = ddesc.find_all('tr')\n",
    "            for i, data_point in enumerate(table_data):\n",
    "                data_point = data_point.get_text()\n",
    "                block_text += data_point + '\\n'\n",
    "    return block_text\n",
    "\n",
    "\n",
    "\n",
    "for keyword in netmed_responses:\n",
    "    try:\n",
    "        soup = BeautifulSoup(netmed_responses[keyword], 'html.parser')\n",
    "        prod_name = soup.find('div', class_='product-right-block').find('div', class_='prodName').find('h1').get_text()\n",
    "        content = soup.find('div', class_='left-block').find_all('div', class_='inner-content')\n",
    "        for block in content:\n",
    "            try:\n",
    "                block_title = block.find('h2').get_text()\n",
    "                # print (block_title)\n",
    "                if 'INTRODUCTION' in block_title:\n",
    "                    introduction = '\\n'.join(para.get_text() for para in block.find_all('p'))\n",
    "                elif 'USES OF' in block_title:\n",
    "                    uses_of = '\\n'.join(para.get_text() for para in block.find_all('li'))\n",
    "                elif 'TABLET WORKS' in block_title:\n",
    "                    how_it_works = '\\n'.join(para.get_text() for para in block.find_all('p'))\n",
    "                elif 'DIRECTIONS FOR USE' in block_title:\n",
    "                    dir_of_use = '\\n'.join(para.get_text() for para in block.find_all('p'))\n",
    "\n",
    "                elif 'UNCOMMON' in block_title:\n",
    "                    uncommon_text = get_li_p_text(block)\n",
    "                elif 'COMMON' in block_title:\n",
    "                    common_text = get_li_p_text(block)                    \n",
    "                elif 'RARE' in block_title:\n",
    "                    rare_text = get_li_p_text(block)\n",
    "                elif 'HOW TO MANAGE' in block_title:\n",
    "                    how_to_manage = '\\n'.join(para.get_text() for para in block.find_all('p'))\n",
    "                elif 'WARNING' in block_title:\n",
    "                    warning_text = ''\n",
    "                    headers = block.find_all('h6')\n",
    "                    content = block.find_all('p')\n",
    "                    for i, header in enumerate(headers):\n",
    "                        warning_text += header.get_text() + '\\n' + content[i].get_text() + '\\n'\n",
    "                    # print(warning_text)\n",
    "                        \n",
    "                elif 'OTHERS' in block_title:\n",
    "                    others_text = get_li_p_text(block)\n",
    "                elif 'INTERACTIONS' in block_title:\n",
    "                    interactions = get_li_p_text(block)\n",
    "                elif 'SYNOPSIS' in block_title:\n",
    "                    synopsis = get_li_p_text(block)\n",
    "                elif 'MORE INFORMATION' in block_title:\n",
    "                    more_info = get_li_p_text(block)\n",
    "                elif 'FAQs' in block_title:\n",
    "                    faqs = get_li_p_text(block)\n",
    "                elif 'REFERENCES' in block_title:\n",
    "                    references = get_li_p_text(block)\n",
    "                elif 'USEFUL DIAGNOSTIC TESTS' in block_title:\n",
    "                    useful_diagnostic_tests = get_li_p_text(block)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        \n",
    "        prescript_content_div = soup.find('div', class_='left-block').find_all('div', class_='drug-content')[1].find('div', class_='prescript-txt')\n",
    "        prescript_content_titles = prescript_content_div.find_all('div', class_='manufacturer_name')\n",
    "        prescript_content_titles.extend(prescript_content_div.find_all('div', class_='manufacturer_address'))\n",
    "        prescript_content_data = prescript_content_div.find_all('div', class_='manufacturer__name_value')\n",
    "        prescript_content_data.extend(prescript_content_div.find_all('div', class_='manufacturer_address_value'))\n",
    "        for i,t in enumerate(prescript_content_titles):\n",
    "            continue\n",
    "            # print (t.get_text())\n",
    "            # print (prescript_content_data[i].get_text())\n",
    "        netmeds_row = {\n",
    "            'prod_name': prod_name,\n",
    "            'introduction': introduction,\n",
    "            'uses_of': uses_of,\n",
    "            'how_it_works': how_it_works,\n",
    "            'dir_of_use': dir_of_use,\n",
    "            'uncommon_text': uncommon_text,\n",
    "            'common_text': common_text,\n",
    "            'rare_text': rare_text,\n",
    "            'how_to_manage': how_to_manage,\n",
    "            'warning_text': warning_text,\n",
    "            'others_text': others_text,\n",
    "            'interactions': interactions,\n",
    "            'synopsis': synopsis, \n",
    "            'more_info': more_info,\n",
    "            'faqs':faqs, \n",
    "            'references': references,\n",
    "            'useful_diagnostic_tests': useful_diagnostic_tests\n",
    "        }\n",
    "        netmeds_data.append(netmeds_row)\n",
    "        # netmeds_dict[keyword] = netmeds_row\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "\n",
    "# Step 4: Save processed data into a new CSV file\n",
    "with open('netmeds1_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['prod_name', 'introduction', 'uses_of','how_it_works','dir_of_use','uncommon_text','common_text','rare_text','how_to_manage','warning_text','others_text','interactions','synopsis','more_info','faqs','references','useful_diagnostic_tests']  # Update with your field names\n",
    "    writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(netmeds_data)\n",
    "\n",
    "print(\"Data saved to netmeds_data.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list to store the concatenated text\n",
    "result = []\n",
    "\n",
    "for block in content:\n",
    "    # Find all h2, p, and ul elements within the block\n",
    "    block_text = \"\"\n",
    "    block_text += block.find('h2').get_text() + \"\\n\" if block.find('h2') else \"\"\n",
    "    block_text += \"\\n\".join(para.get_text() for para in block.find_all('p')) + \"\\n\"\n",
    "    block_text += \"\\n\".join(li.get_text() for ul in block.find_all('ul') for li in ul.find_all('li')) + \"\\n\"\n",
    "    \n",
    "    # Append the concatenated text to the result list\n",
    "    result.append(block_text)\n",
    "\n",
    "# Print or store the result\n",
    "for text in result:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing(block):\n",
    "    # Define a list to store the concatenated text\n",
    "    result = []\n",
    "\n",
    "    for block in content:\n",
    "        # Find all h2, p, and ul elements within the block\n",
    "        block_text = \"\"\n",
    "        block_text += block.find('h2').get_text() + \"\\n\" if block.find('h2') else \"\"\n",
    "        block_text += \"\\n\".join(para.get_text() for para in block.find_all('p')) + \"\\n\"\n",
    "        block_text += \"\\n\".join(li.get_text() for ul in block.find_all('ul') for li in ul.find_all('li')) + \"\\n\"\n",
    "        \n",
    "        # Append the concatenated text to the result list\n",
    "        result.append(block_text)\n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
